{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UlcMTNgEuUTS"
      },
      "outputs": [],
      "source": [
        "#Tamim Mahmud From Daffodil International University\n",
        "#19/06/2025"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax à¦•à§‡à¦¨ à¦²à¦¾à¦—à§‡?\n",
        "\n",
        "à¦§à¦°à§‹ à¦¤à§à¦®à¦¿ à¦à¦•à¦Ÿà¦¾ multi-class classification à¦•à¦°à¦¤à§‡à¦›à§‹ â€” à¦¯à§‡à¦®à¦¨ 3à¦Ÿà¦¾ class:\n",
        "\n",
        "ðŸ¶ Dog\n",
        "\n",
        "\n",
        "ðŸ± Cat\n",
        "\n",
        "\n",
        "ðŸ° Rabbit\n",
        "\n",
        "\n",
        "à¦¤à§‹à¦®à¦¾à¦° model à¦à¦° à¦¶à§‡à¦· à¦²à§‡à§Ÿà¦¾à¦°à§‡ à¦¯à¦¾ à¦¬à§‡à¦° à¦¹à§Ÿ, à¦¤à¦¾à¦•à§‡ à¦¬à¦²à§‡ logits à¦¬à¦¾ raw scoresà¥¤ à¦¯à§‡à¦®à¦¨:\n",
        "\n",
        "\n",
        "scores = [2.0, 1.0, 0.1]\n",
        "à¦à¦‡à¦—à§à¦²à§‹ à¦•à§‹à¦¨à§‹ probability à¦¨à¦¾ â€” à¦¬à¦°à¦‚ \"confidence\" à¦à¦° à¦®à¦¤à¥¤\n",
        "\n",
        "Softmax à¦à¦‡ raw scores à¦—à§à¦²à§‹à¦•à§‡ probability à¦¤à§‡ convert à¦•à¦°à§‡ à¦¦à§‡à§Ÿ â€” à¦à¦®à¦¨à¦­à¦¾à¦¬à§‡ à¦¯à§‡à¦¨:\n",
        "\n",
        "à¦¸à¦¬à¦—à§à¦²à¦¾ value à¦¥à¦¾à¦•à§‡ 0 à¦¥à§‡à¦•à§‡ 1 à¦à¦° à¦®à¦§à§à¦¯à§‡\n",
        "\n",
        "à¦¸à¦¬à¦—à§à¦²à¦¾ à¦¯à§‹à¦— à¦•à¦°à¦²à§‡ à¦¹à§Ÿ 1"
      ],
      "metadata": {
        "id": "d-Y3tjlm2Ddw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model's raw scores (logits) for 3 classes\n",
        "scores = torch.tensor([2.0, 1.0, 0.1])\n",
        "\n",
        "# Apply softmax\n",
        "probabilities = F.softmax(scores, dim=0)\n",
        "\n",
        "print(\"Probabilities:\", probabilities)\n",
        "print(\"Sum:\", probabilities.sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lC7y5XJuh92",
        "outputId": "b0ef14a6-feae-4233-9eab-4e08b523a3a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: tensor([0.6590, 0.2424, 0.0986])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "score=torch.tensor([1.0,3.0,2.0,4.0,6.0])\n",
        "\n",
        "probabilities=F.softmax(score,dim=0) #Converts scores â†’ probabilities\n",
        "\n",
        "print(\"Probabilities:\",probabilities)\n",
        "print(\"Sum:\",probabilities.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k41895LwuzQb",
        "outputId": "f6aedb12-5877-418a-9356-47bac7e77746"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: tensor([0.0056, 0.0411, 0.0151, 0.1118, 0.8263])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 : proti ta score er e power x beboahr kori\n",
        "\n",
        "setp 2: shob gulo ke jug kori\n",
        "\n",
        "step3: à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ à¦¸à§à¦•à§‹à¦° à¦­à¦¾à¦— à¦•à¦°à¦¿ à¦ à¦¯à§‹à¦—à¦«à¦² à¦¦à¦¿à§Ÿà§‡\n",
        "\n",
        "step 4: ja ashe shb jug krle asn ashbe 1"
      ],
      "metadata": {
        "id": "SP7OjuW03seP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  #CrossEntropyLoss à¦•à¦¿?\n",
        "#PyTorch à¦ CrossEntropyLoss à¦®à¦¾à¦¨à§‡:\n",
        "\n",
        "#ðŸ”¥ Softmax + Log + Negative Log Likelihood (NLL) â€” à¦¸à¦¬ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡ à¦•à¦°à§‡ à¦«à§‡à¦²à§‡!"
      ],
      "metadata": {
        "id": "dzK6LXCx0uuD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scores = [2.0, 1.0, 0.1]\n",
        "\n",
        "Softmax output = [0.659, 0.242, 0.099]\n",
        "\n",
        "now ,\n",
        ": Take Negative Log of True class\n",
        "\n",
        "Loss=âˆ’log(0.659)=0.417\n",
        "\n",
        "à¦à¦Ÿà¦¾à¦‡ CrossEntropyLoss!"
      ],
      "metadata": {
        "id": "5BUXkCFc4kSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#softmax->Probability à¦¬à§‡à¦° à¦•à¦°à§‡\n",
        "#-log(true_class_prob)->Loss à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à§‡\n",
        "#CrossEntropyLoss->\tà¦¸à¦¬à¦•à¦¿à¦›à§ à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦•à¦°à§‡ à¦¦à§‡à§Ÿ"
      ],
      "metadata": {
        "id": "FspvRZ9U0z6N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# raw model output\n",
        "scores = torch.tensor([2.0, 10.0, 0.1])  # logits\n",
        "labels = torch.tensor([0])             # correct class index\n",
        "\n",
        "# Method 1: manual softmax + log + loss\n",
        "softmax = F.softmax(scores, dim=0)\n",
        "log_softmax = torch.log(softmax)\n",
        "loss_manual = -log_softmax[labels]\n",
        "print(\"Manual Loss:\", loss_manual.item())\n",
        "\n",
        "# Method 2: use CrossEntropyLoss directly (preferred)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# CrossEntropyLoss expects input as [batch_size, num_classes]\n",
        "# so we need to reshape scores\n",
        "scores_batch = scores.unsqueeze(0)  # shape: [1, 3]\n",
        "loss = loss_fn(scores_batch, labels)\n",
        "print(\"CrossEntropyLoss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE4xuIgM5NZ_",
        "outputId": "d39dc126-76b6-4c6f-ea2e-97a0026c8a66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Loss: 8.000385284423828\n",
            "CrossEntropyLoss: 8.000385284423828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "s=torch.tensor([20.0,4.0,6.0,8.0])\n",
        "\n",
        "softmax=F.softmax(s,dim=0)\n",
        "print(\"Probabilities:\",softmax)\n",
        "print(\"Sum:\",softmax.sum())\n",
        "\n",
        "\n",
        "labels=torch.tensor([0])\n",
        "logg=torch.log(softmax)\n",
        "loss_manual=-logg[labels]\n",
        "\n",
        "print(\"Loss Manual:\",loss_manual.item())\n",
        "\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "score_batch=s.unsqueeze(0)\n",
        "loss=loss_fn(score_batch,labels)\n",
        "print(\"CrossEntropyLoss:\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87W7DwUr6ZU0",
        "outputId": "c9bd6f98-b08a-468d-c879-5d6614596466"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: tensor([9.9999e-01, 1.1253e-07, 8.3152e-07, 6.1442e-06])\n",
            "Sum: tensor(1.0000)\n",
            "Loss Manual: 7.152582838898525e-06\n",
            "CrossEntropyLoss: 7.152531907195225e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch-à¦ nn.CrossEntropyLoss batch of samples à¦¨à¦¿à§Ÿà§‡ à¦•à¦¾à¦œ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤\n",
        "à¦à¦Ÿà¦¿ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ à¦¸à§à¦¯à¦¾à¦®à§à¦ªà¦²à§‡à¦° à¦œà¦¨à§à¦¯ loss à¦¬à§‡à¦° à¦•à¦°à§‡, à¦¤à¦¾à¦°à¦ªà¦° à¦¸à§‡à¦—à§à¦²à§‹à¦° average à¦¨à§‡à§Ÿ"
      ],
      "metadata": {
        "id": "G5ky5Qe882eV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxbgR0bJ7uz5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Logits: raw model output (batch_size = 3, num_classes = 3)\n",
        "logits = torch.tensor([[2.0, 1.0, 0.1],   # True class: 0\n",
        "                       [1.0, 3.0, 0.1],   # True class: 1\n",
        "                       [0.2, 0.3, 0.5]])  # True class: 2\n",
        "\n",
        "labels = torch.tensor([0, 1, 2])\n",
        "\n",
        "# PyTorch built-in CrossEntropyLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(logits, labels)\n",
        "print(\"ðŸ”¹ CrossEntropy Loss:\", loss.item())\n",
        "\n",
        "# Manually calculating softmax per row (dim=1)\n",
        "softmax = F.softmax(logits, dim=1)\n",
        "print(\"ðŸ”¸ Probabilities:\\n\", softmax)\n",
        "\n",
        "# Get probability of correct class for each sample\n",
        "probs_of_correct = softmax[torch.arange(3), labels]\n",
        "print(\"âœ… Prob of correct class per sample:\", probs_of_correct)\n",
        "\n",
        "# Manual loss = -log(prob)\n",
        "loss_manual = -torch.log(probs_of_correct)\n",
        "print(\"ðŸ“‰ Manual Loss per sample:\", loss_manual)\n",
        "\n",
        "# Mean of all sample losses (same as nn.CrossEntropyLoss)\n",
        "manual_loss_mean = loss_manual.mean()\n",
        "print(\"ðŸ“Š Final Manual Loss:\", manual_loss_mean.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFmkxU5m-Ni6",
        "outputId": "bb248b2d-5a14-4b0b-8e88-151576e7a1e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ CrossEntropy Loss: 0.5103718638420105\n",
            "ðŸ”¸ Probabilities:\n",
            " tensor([[0.6590, 0.2424, 0.0986],\n",
            "        [0.1137, 0.8401, 0.0462],\n",
            "        [0.2894, 0.3199, 0.3907]])\n",
            "âœ… Prob of correct class per sample: tensor([0.6590, 0.8401, 0.3907])\n",
            "ðŸ“‰ Manual Loss per sample: tensor([0.4170, 0.1743, 0.9398])\n",
            "ðŸ“Š Final Manual Loss: 0.5103718638420105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JeUYUNDWKAs2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXOeadaNKA-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x),axis=0)\n",
        "\n",
        "x=np.array([2.0,1.0,0.1])\n",
        "\n",
        "re=softmax(x)\n",
        "print(\"softmax:\",re)\n",
        "print(\"sum:\",re.sum())\n",
        "\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "outputs = torch.softmax(x, dim=0)\n",
        "print('softmax torch:', outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LidAzfBP-w09",
        "outputId": "b7936696-6ad0-4dc8-8bc0-ed223f34107b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax: [0.65900114 0.24243297 0.09856589]\n",
            "sum: 1.0\n",
            "softmax torch: tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "#\n",
        "#        -> 2.0              -> 0.65\n",
        "# Linear -> 1.0  -> Softmax  -> 0.25   -> CrossEntropy(y, y_hat)\n",
        "#        -> 0.1              -> 0.1\n",
        "#\n",
        "#     scores(logits)      probabilities\n",
        "#                           sum = 1.0\n",
        "#\n",
        "\n",
        "# Softmax applies the exponential function to each element, and normalizes\n",
        "# by dividing by the sum of all these exponentials\n",
        "# -> squashes the output to be between 0 and 1 = probability\n",
        "# sum of all probabilities is 1\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:', outputs)\n",
        "\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "outputs = torch.softmax(x, dim=0) # along values along first axis\n",
        "print('softmax torch:', outputs)\n",
        "\n",
        "# Cross entropy\n",
        "# Cross-entropy loss, or log loss, measures the performance of a classification model\n",
        "# whose output is a probability value between 0 and 1.\n",
        "# -> loss increases as the predicted probability diverges from the actual label\n",
        "def cross_entropy(actual, predicted):\n",
        "    EPS = 1e-15\n",
        "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
        "    loss = -np.sum(actual * np.log(predicted))\n",
        "    return loss # / float(predicted.shape[0])\n",
        "\n",
        "# y must be one hot encoded\n",
        "# if class 0: [1 0 0]\n",
        "# if class 1: [0 1 0]\n",
        "# if class 2: [0 0 1]\n",
        "Y = np.array([1, 0, 0])\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "l1 = cross_entropy(Y, Y_pred_good)\n",
        "l2 = cross_entropy(Y, Y_pred_bad)\n",
        "print(f'Loss1 numpy: {l1:.4f}')\n",
        "print(f'Loss2 numpy: {l2:.4f}')\n",
        "\n",
        "# CrossEntropyLoss in PyTorch (applies Softmax)\n",
        "# nn.LogSoftmax + nn.NLLLoss\n",
        "# NLLLoss = negative log likelihood loss\n",
        "loss = nn.CrossEntropyLoss()\n",
        "# loss(input, target)\n",
        "\n",
        "# target is of size nSamples = 1\n",
        "# each element has class label: 0, 1, or 2\n",
        "# Y (=target) contains class labels, not one-hot\n",
        "Y = torch.tensor([0])\n",
        "\n",
        "# input is of size nSamples x nClasses = 1 x 3\n",
        "# y_pred (=input) must be raw, unnormalizes scores (logits) for each class, not softmax\n",
        "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
        "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(f'PyTorch Loss1: {l1.item():.4f}')\n",
        "print(f'PyTorch Loss2: {l2.item():.4f}')\n",
        "\n",
        "# get predictions\n",
        "_, predictions1 = torch.max(Y_pred_good, 1)\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "print(f'Actual class: {Y.item()}, Y_pred1: {predictions1.item()}, Y_pred2: {predictions2.item()}')\n",
        "\n",
        "# allows batch loss for multiple samples\n",
        "\n",
        "# target is of size nBatch = 3\n",
        "# each element has class label: 0, 1, or 2\n",
        "Y = torch.tensor([2, 0, 1])\n",
        "\n",
        "# input is of size nBatch x nClasses = 3 x 3\n",
        "# Y_pred are logits (not softmax)\n",
        "Y_pred_good = torch.tensor(\n",
        "    [[0.1, 0.2, 3.9], # predict class 2\n",
        "    [1.2, 0.1, 0.3], # predict class 0\n",
        "    [0.3, 2.2, 0.2]]) # predict class 1\n",
        "\n",
        "Y_pred_bad = torch.tensor(\n",
        "    [[0.9, 0.2, 0.1],\n",
        "    [0.1, 0.3, 1.5],\n",
        "    [1.2, 0.2, 0.5]])\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "print(f'Batch Loss1:  {l1.item():.4f}')\n",
        "print(f'Batch Loss2: {l2.item():.4f}')\n",
        "\n",
        "# get predictions\n",
        "_, predictions1 = torch.max(Y_pred_good, 1)\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "print(f'Actual class: {Y}, Y_pred1: {predictions1}, Y_pred2: {predictions2}')\n",
        "\n",
        "# Binary classification\n",
        "class NeuralNet1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(NeuralNet1, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # sigmoid at the end\n",
        "        y_pred = torch.sigmoid(out)\n",
        "        return y_pred\n",
        "\n",
        "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Multiclass problem\n",
        "class NeuralNet2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet2, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # no softmax at the end\n",
        "        return out\n",
        "\n",
        "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss()  # (applies Softmax)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-UAS9-jKB00",
        "outputId": "b2bf14d6-2274-43e3-cb49-8035923775ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "softmax torch: tensor([0.6590, 0.2424, 0.0986])\n",
            "Loss1 numpy: 0.3567\n",
            "Loss2 numpy: 2.3026\n",
            "PyTorch Loss1: 0.4170\n",
            "PyTorch Loss2: 1.8406\n",
            "Actual class: 0, Y_pred1: 0, Y_pred2: 1\n",
            "Batch Loss1:  0.2834\n",
            "Batch Loss2: 1.6418\n",
            "Actual class: tensor([2, 0, 1]), Y_pred1: tensor([2, 0, 1]), Y_pred2: tensor([0, 2, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YfpMnVni81jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o29y-7oEKofg"
      }
    }
  ]
}